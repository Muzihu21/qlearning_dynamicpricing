# -*- coding: utf-8 -*-
"""train_qlearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W863RgoMLCIOFmb-dbt93OUDrlT09me4
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from q_learning_env import PenjualanEnv
from itertools import product

# ===========================
# Hyperparameters
# ===========================
alpha = 0.1
gamma = 0.9
epsilon = 1.0          # eksplorasi penuh di awal
epsilon_min = 0.05     # batas eksplorasi
decay_rate = 0.995     # laju penurunan eksplorasi
episodes = 10000

# ===========================
# Init environment
# ===========================
env = PenjualanEnv()
state_to_index = env.state_to_index
n_states = len(state_to_index)
q_table = np.zeros((n_states, env.n_actions))

rewards_per_episode = []

env.unique_harga = sorted(env.data['harga_index'].unique())
env.unique_penjualan = sorted(env.data['penjualan_level'].unique())
env.states = list(product(env.unique_harga, env.unique_penjualan))
env.state_to_index = {s: i for i, s in enumerate(env.states)}
state_to_index = env.state_to_index
n_states = len(state_to_index)
q_table = np.zeros((n_states, env.n_actions))
# ===========================
# Training loop
# ===========================
for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        state_idx = state_to_index[state]

        # Epsilon-greedy policy
        if np.random.rand() < epsilon:
            action = np.random.randint(env.n_actions)
        else:
            action = np.argmax(q_table[state_idx])

        next_state, reward, done = env.step(action)
        next_state_idx = state_to_index[next_state]

        # Q-learning update
        q_table[state_idx, action] += alpha * (
            reward + gamma * np.max(q_table[next_state_idx]) - q_table[state_idx, action]
        )

        state = next_state
        total_reward += reward

    # Decay epsilon
    if epsilon > epsilon_min:
        epsilon *= decay_rate

    rewards_per_episode.append(total_reward)

    if episode % 100 == 0:
        print(f"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.4f}")

# ===========================
# Save Results
# ===========================
os.makedirs("visualizations", exist_ok=True)

np.save("q_table.npy", q_table)
np.save("rewards_per_episode.npy", rewards_per_episode)

# ===========================
# Visualisasi Reward
# ===========================
plt.figure(figsize=(10, 5))
plt.plot(rewards_per_episode)
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("Q-Learning Training Reward")
plt.grid(True)
plt.savefig("visualizations/reward_plot.png")
plt.close()

# ===========================
# Reward Aggregasi 100-an
# ===========================
df_reward = pd.DataFrame({
    "Episode": range(1, len(rewards_per_episode)+1),
    "Reward": rewards_per_episode
})
df_reward["Batch"] = ((df_reward["Episode"] - 1) // 100) * 100 + 1
reward_aggregated = df_reward.groupby("Batch")["Reward"].sum().reset_index()
total_omzet_training = df_reward["Reward"].sum()

plt.figure(figsize=(10, 5))
plt.plot(reward_aggregated["Batch"], reward_aggregated["Reward"], marker='o', color='royalblue')
plt.title(f"Tren Reward Selama Training\nTotal Omzet: Rp {total_omzet_training:,.0f}")
plt.xlabel("Batch Episode")
plt.ylabel("Total Reward")
plt.grid(True)
plt.tight_layout()
plt.savefig("visualizations/reward_trend.png")
plt.close()

# ===========================
# Perbandingan Random vs Q-Learning
# ===========================
agent_rewards = []
random_rewards = []
episodes_eval = 100

for _ in range(episodes_eval):
    # Q-Learning agent
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        state_idx = state_to_index[state]
        action = np.argmax(q_table[state_idx])
        state, reward, done = env.step(action)
        total_reward += reward
    agent_rewards.append(total_reward)

    # Random agent
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = np.random.randint(env.n_actions)
        state, reward, done = env.step(action)
        total_reward += reward
    random_rewards.append(total_reward)

plt.figure(figsize=(8, 6))
sns.boxplot(data=[random_rewards, agent_rewards], palette=["salmon", "skyblue"])
plt.xticks([0, 1], ["Tanpa Strategi", "Q-Learning Agent"])
plt.ylabel("Total Reward (Omzet)")
plt.title("\ud83d\udcca Perbandingan Omzet\nRandom vs Q-Learning")
plt.grid(True)
plt.tight_layout()
plt.savefig("visualizations/compare_omzet_boxplot.png")
plt.close()
# Ringkasan
# ===========================
print(f"Rata-rata Random : Rp {np.mean(random_rewards):,.0f}")
print(f"Rata-rata Q-Learn: Rp {np.mean(agent_rewards):,.0f}")
print("\n\u2705 Q-table dan reward per episode berhasil disimpan.")
